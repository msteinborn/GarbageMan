"""
RAG Service for Business Terms
Standalone service for retrieval augmented generation
"""

import os
import json
from typing import List, Dict
import requests
from sentence_transformers import SentenceTransformer
import chromadb

# Initialize embedding model
EMBEDDING_MODEL = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize Chroma (local vector database) - using new API
CHROMA_DIR = "./chroma_data"
if not os.path.exists(CHROMA_DIR):
    os.makedirs(CHROMA_DIR)

client = chromadb.PersistentClient(path=CHROMA_DIR)


def fetch_business_terms_dataset() -> List[Dict]:
    """
    Fetch business terms dataset from a public source
    Using a GitHub raw content as data source
    """
    print("ğŸ“¥ Fetching business terms dataset...")
    
    # Using a public business glossary CSV
    url = "https://raw.githubusercontent.com/daviskernel/business-glossary/main/glossary.csv"
    
    try:
        # Try primary source
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # Parse CSV
        lines = response.text.strip().split('\n')
        terms = []
        
        # Skip header
        for line in lines[1:]:
            parts = line.split(',', 1)
            if len(parts) == 2:
                term, definition = parts
                terms.append({
                    "term": term.strip().strip('"'),
                    "definition": definition.strip().strip('"')
                })
        
        print(f"âœ“ Loaded {len(terms)} business terms")
        return terms
    
    except Exception as e:
        print(f"âš ï¸  Primary source failed: {e}")
        print("ğŸ“¥ Using fallback dataset...")
        return get_fallback_business_terms()


def get_fallback_business_terms() -> List[Dict]:
    """Fallback dataset if online fetch fails"""
    return [
        {"term": "ROI", "definition": "Return on Investment - a measure of profit relative to investment"},
        {"term": "KPI", "definition": "Key Performance Indicator - measurable value showing effectiveness"},
        {"term": "EBITDA", "definition": "Earnings Before Interest, Taxes, Depreciation, and Amortization"},
        {"term": "Cash Flow", "definition": "Movement of money in and out of a business"},
        {"term": "Market Cap", "definition": "Total market value of a company's shares"},
        {"term": "Revenue", "definition": "Total income generated by sales of goods/services"},
        {"term": "P&L", "definition": "Profit and Loss statement showing financial performance"},
        {"term": "Amortization", "definition": "Spreading cost of intangible asset over useful life"},
        {"term": "Burn Rate", "definition": "Rate at which company spends cash reserves"},
        {"term": "Runway", "definition": "Time a company can operate with current cash"},
        {"term": "IPO", "definition": "Initial Public Offering - first time company sells shares publicly"},
        {"term": "Equity", "definition": "Ownership stake in a company"},
        {"term": "Debt", "definition": "Money borrowed that must be repaid with interest"},
        {"term": "Asset", "definition": "Anything of value owned by a business"},
        {"term": "Liability", "definition": "Financial obligation or debt owed by business"},
    ]


def create_vector_store(terms: List[Dict]):
    """
    Create vector embeddings and store in Chroma
    """
    print("ğŸ”„ Creating vector embeddings...")
    
    # Create collection
    collection = client.get_or_create_collection(
        name="business_terms",
        metadata={"hnsw:space": "cosine"}
    )
    
    # Prepare data
    ids = []
    documents = []
    metadatas = []
    embeddings = []
    
    for idx, item in enumerate(terms):
        term = item["term"]
        definition = item["definition"]
        
        # Create document: term + definition for better context
        doc = f"Term: {term}\nDefinition: {definition}"
        
        # Generate embedding
        embedding = EMBEDDING_MODEL.encode(doc).tolist()
        
        ids.append(f"term_{idx}")
        documents.append(doc)
        embeddings.append(embedding)
        metadatas.append({
            "term": term,
            "definition": definition
        })
    
    # Add to collection
    collection.add(
        ids=ids,
        embeddings=embeddings,
        documents=documents,
        metadatas=metadatas
    )
    
    print(f"âœ“ Stored {len(terms)} terms in vector database")
    return collection


def retrieve_relevant_terms(query: str, collection, top_k: int = 3) -> List[Dict]:
    """
    Retrieve relevant business terms for a query
    """
    # Embed the query
    query_embedding = EMBEDDING_MODEL.encode(query).tolist()
    
    # Search in collection
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    # Format results
    retrieved = []
    if results['metadatas'] and results['metadatas'][0]:
        for i, metadata in enumerate(results['metadatas'][0]):
            retrieved.append({
                "term": metadata["term"],
                "definition": metadata["definition"],
                "distance": results['distances'][0][i] if 'distances' in results else 0
            })
    
    return retrieved


def generate_rag_response(query: str, retrieved_terms: List[Dict]) -> str:
    """
    Generate response using RAG (Retrieved context + LLM)
    For now, formats retrieved context as structured response
    """
    response = f"Query: {query}\n\n"
    response += "ğŸ“š Retrieved Business Terms:\n"
    response += "=" * 50 + "\n"
    
    if retrieved_terms:
        for item in retrieved_terms:
            response += f"\nğŸ“Œ {item['term']}\n"
            response += f"   {item['definition']}\n"
    else:
        response += "No matching terms found.\n"
    
    return response


def main():
    """Main RAG pipeline"""
    print("\nğŸš€ Business Terms RAG Service\n")
    
    # Step 1: Fetch dataset
    terms = fetch_business_terms_dataset()
    
    # Step 2: Create vector store
    collection = create_vector_store(terms)
    
    # Step 3: Interactive retrieval
    print("\nğŸ’¬ Enter queries (type 'exit' to quit):\n")
    
    while True:
        query = input("Query: ").strip()
        
        if query.lower() == 'exit':
            print("Goodbye! ğŸ‘‹")
            break
        
        if not query:
            continue
        
        # Retrieve relevant terms
        retrieved = retrieve_relevant_terms(query, collection, top_k=3)
        
        # Generate response
        response = generate_rag_response(query, retrieved)
        print(response)
        print("-" * 50 + "\n")


if __name__ == "__main__":
    main()
